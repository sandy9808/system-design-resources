Bloom Filters are space-efficient probabilistic data structures that test whether an element is a member of a set with possible false positives (saying element exists when it doesn't) but no false negatives (never saying element doesn't exist when it does), using multiple hash functions to map elements to bit positions in fixed-size bit array where adding element sets all corresponding bits to 1 and checking membership returns true only if all corresponding bits are 1, achieving constant-time O(k) operations where k is number of hash functions regardless of set size, consuming far less memory than storing actual elements (tens of bits per element vs hundreds of bytes), with false positive rate controlled by array size (m bits), number of elements (n), and number of hash functions (k) following formula FPR â‰ˆ (1 - e^(-kn/m))^k typically configured for 1-5% false positive rate balancing space and accuracy, commonly used in databases to avoid expensive disk lookups by maintaining bloom filter in memory checking if key might exist before querying disk saving I/O when key definitely doesn't exist, content delivery networks avoiding caching of one-hit wonders by only caching content that passes bloom filter check indicating previous access, distributed systems checking if other nodes have data before making network calls, blockchain wallets quickly checking if transaction involves wallet addresses, spell checkers identifying definitely misspelled words without dictionary lookup, and weak password detection flagging likely weak passwords against compromised password database, implemented using variants like Counting Bloom Filters supporting deletions by using counters instead of bits, Scalable Bloom Filters dynamically growing by adding filters as set grows, and Cuckoo Filters providing similar benefits with deletion support and better space efficiency, with trade-offs including false positives requiring handling in application logic through fallback checks, inability to list elements or iterate through set, fixed size requiring capacity planning upfront or scalable variants, and optimal parameter selection depending on expected cardinality and acceptable error rate balancing memory usage with accuracy.
