Fault tolerance is a system's capability to continue operating correctly and providing service even when some components fail, using redundancy, error detection, error recovery, and graceful degradation to maintain functionality despite hardware failures, software bugs, network partitions, or data corruption. Building fault-tolerant systems requires designing for failure as the normal case rather than an exception, accepting that in large distributed systems with thousands of servers and network components, something is always failing somewhere at any given time, making fault tolerance essential rather than optional. Redundancy forms the foundation of fault tolerance through hardware redundancy using redundant servers, network paths, power supplies, and storage devices, software redundancy through multiple instances of services running simultaneously, data redundancy via replication across multiple nodes, and temporal redundancy where failed operations are retried after delays. Error detection mechanisms identify faults through checksums detecting data corruption during transmission or storage, heartbeat messages between distributed components, health checks verifying service responsiveness, exception handling catching software errors, and anomaly detection using machine learning to identify unusual patterns indicating degraded components. Error recovery strategies include retry mechanisms with exponential backoff for transient failures, circuit breakers preventing cascading failures by stopping requests to failing services, fallback mechanisms providing degraded functionality when primary systems fail, compensation transactions rolling back distributed operations when later steps fail, and checkpointing saving system state periodically to enable recovery from known good points. Replication provides fault tolerance for data and services through primary-backup replication where writes go to primary with asynchronous replication to backups, chain replication where writes propagate through a chain of replicas providing strong consistency, and quorum-based replication requiring majority agreement before acknowledging writes ensuring availability despite minority failures. Consensus protocols enable fault-tolerant coordination in distributed systems using Paxos and Raft algorithms that guarantee agreement on values even when some nodes fail or messages are lost, Byzantine Fault Tolerance (BFT) protocols handling malicious nodes that send incorrect information, and leader election mechanisms ensuring exactly one coordinator exists despite network partitions. Graceful degradation allows systems to provide reduced functionality when components fail rather than complete outage, implementing this through feature flags disabling non-essential features during high load or partial failures, read-only mode allowing queries while blocking updates when database primaries fail, cached data serving slightly stale information when live data sources are unavailable, and service level prioritization where critical operations continue while nice-to-have features are disabled. Partial failure handling in microservices requires bulkhead patterns isolating resources for different services preventing one service failure from consuming all resources, timeout configuration ensuring failed services don't cause indefinite waits, and idempotency allowing safe retry of operations without duplicate effects. State management impacts fault tolerance where stateless services can easily be replaced when they fail making them inherently more fault-tolerant, while stateful services require careful session replication, distributed state stores, or state externalization to databases or caches. Testing fault tolerance requires chaos engineering injecting random failures into production systems, fault injection testing where specific failure scenarios are simulated, game day exercises conducting coordinated failure scenarios involving multiple teams, and disaster recovery drills testing complete datacenter or region failures. Trade-offs in fault tolerance design include performance overhead from redundancy and synchronization, increased complexity in error handling and recovery logic, higher infrastructure costs from maintaining redundant systems, consistency challenges when replicas diverge, and the impossibility of tolerating all possible failures requiring prioritization based on likelihood and impact. CAP theorem constrains fault tolerance in distributed systems proving that during network partitions systems must choose between consistency maintaining identical data across replicas or availability continuing to serve requests, making fault tolerance often mean accepting eventual consistency where replicas temporarily diverge but eventually converge. Failure modes that fault-tolerant systems must handle include crash failures where processes or machines suddenly stop, omission failures where messages are lost in transit, timing failures where operations take longer than expected potentially causing timeouts, Byzantine failures where components exhibit arbitrary incorrect behavior potentially from bugs or malicious actors, and correlated failures where multiple components fail simultaneously due to shared dependencies like power systems or software bugs. Measuring fault tolerance uses metrics like Mean Time Between Failures (MTBF) measuring average operational time before failures, Mean Time To Repair (MTTR) measuring average time to restore service, availability percentage calculated as uptime divided by total time typically expressed as nines (99.9% is three nines), and blast radius measuring how many users or services are impacted by individual component failures. Real-world fault tolerance examples include RAID storage tolerating disk failures through parity or mirroring, aircraft flight control systems using triple modular redundancy comparing outputs from three independent computers, banking systems using dual data centers with synchronous replication, and content delivery networks routing around failed edge servers while maintaining service availability.