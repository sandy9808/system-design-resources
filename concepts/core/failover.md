Failover is an automated or manual process that switches system operations from a failed primary component to a redundant secondary component to maintain service availability and minimize downtime during hardware failures, software crashes, or network outages. In distributed systems, failover mechanisms operate at multiple layers including database failover where a standby replica is promoted to primary when the master fails, load balancer failover using protocols like VRRP (Virtual Router Redundancy Protocol) or keepalived to maintain IP address availability, application server failover through container orchestration systems like Kubernetes that restart failed pods, and network failover using redundant switches and routers with dynamic routing protocols. Failover configurations typically use active-passive mode where a secondary system remains idle until needed, active-active mode where multiple systems process requests simultaneously and take over each other's load during failures, or N+1 redundancy where N active systems are supported by one standby capable of handling any single failure. Detection mechanisms are critical for triggering failover and include heartbeat monitoring where systems send periodic health signals with failure detected after missing consecutive heartbeats typically within 5-30 seconds, health check endpoints that application load balancers poll to verify service responsiveness, TCP connection monitoring to detect network partition or process crashes, and application-level health checks that verify database connectivity and external dependencies. Automatic failover systems use consensus protocols like Raft or Paxos in distributed databases, quorum-based decision making where multiple nodes vote on promoting a new leader to prevent split-brain scenarios where multiple nodes simultaneously believe they are primary, and fencing mechanisms that ensure failed nodes cannot corrupt data by using techniques like STONITH (Shoot The Other Node In The Head) or distributed locks. Database failover specifically involves promoting a read replica to master, redirecting writes from the failed primary, ensuring data consistency through techniques like synchronous replication where writes are confirmed on both primary and replica before acknowledging to clients, semi-synchronous replication as a compromise between performance and durability, or asynchronous replication accepting potential data loss of recent uncommitted transactions measured by RPO (Recovery Point Objective). DNS-based failover uses health checks to update DNS records pointing to healthy servers, though this approach suffers from DNS caching delays where clients and intermediate resolvers may continue using cached records for minutes or hours based on TTL settings, making it suitable only for failures lasting longer than DNS propagation time. Application-level failover requires session persistence considerations where stateful applications must replicate session data to standby instances using sticky sessions with session replication, centralized session stores in Redis or databases, or stateless design using JWT tokens where all necessary information travels with each request. Failover testing through regular drills and chaos engineering practices like Netflix's Chaos Monkey ensures mechanisms work correctly under actual failure conditions, identifying issues like misconfigured health checks, insufficient replica capacity, or missing database permissions that only surface during actual failover events. Trade-offs in failover design include the balance between detection speed and false positive rate where faster detection may trigger unnecessary failovers due to transient network issues, the overhead of keeping standby systems synchronized and ready, potential data loss in asynchronous replication configurations, and the complexity of managing split-brain prevention. Recovery time objective (RTO) defines acceptable downtime duration and drives failover design decisions, with hot standby configurations achieving RTO under one minute but requiring continuously synchronized systems, warm standby taking several minutes to restore service from recent backups or snapshots, and cold standby potentially taking hours to provision and configure new infrastructure. Challenges in failover implementation include ensuring data consistency during the transition especially for distributed transactions spanning multiple services, handling in-flight requests that were being processed during failure, cascading failures where overloaded standby systems fail after taking over full load, and maintaining system observability during chaotic failover events when logging and monitoring infrastructure may also be impaired. Cloud provider managed services often include built-in failover capabilities such as AWS RDS Multi-AZ deployments that automatically failover databases across availability zones, Azure SQL Database with automatic failover groups, and Google Cloud SQL high availability configurations, trading some control and visibility for reduced operational burden. Manual failover procedures remain important as fallback when automatic systems fail or for planned maintenance, requiring clear runbooks, proper access controls, communication protocols, and rollback procedures if failover introduces new issues. Monitoring failover systems requires tracking metrics like time-to-detect-failure measured from when failure occurs to when detection systems recognize it, time-to-failover measuring the complete transition duration, failover success rate, false positive rate where healthy systems are incorrectly failed over, and data loss measured in transactions or time duration for RPO compliance.