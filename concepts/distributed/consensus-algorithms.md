Consensus Algorithms enable distributed systems to agree on shared state despite failures using protocols like Paxos establishing consensus through prepare and accept phases where proposer selects proposal number and requests promises from majority of acceptors who promise not to accept lower-numbered proposals, proposer then sends accept request with value, acceptors accept if they haven't promised higher number, consensus reached when majority accepts providing strong consistency but suffering from complexity in implementation and liveness issues under certain failure scenarios, Multi-Paxos optimizing by electing stable leader reducing message overhead for subsequent proposals, Raft simplifying Paxos concepts through leader election using randomized timeouts where followers become candidates after election timeout, collect votes from majority, and serve as leader coordinating log replication, leaders append entries to their log and replicate to followers, commit entry when replicated to majority, followers apply committed entries to state machines maintaining identical logs across cluster, providing understandability through clear leader role and log structure while achieving equivalent guarantees to Paxos, implemented in etcd, Consul, and CockroachDB, ZAB (Zookeeper Atomic Broadcast) used by Zookeeper ensuring total ordering of writes through leader-based protocol similar to Raft with additional recovery phase guaranteeing transactions applied in order, Byzantine Fault Tolerance (BFT) algorithms like PBFT tolerating malicious nodes not just failures requiring 3f+1 nodes to tolerate f byzantine nodes where nodes exchange messages in pre-prepare, prepare, and commit phases, reaching agreement when 2f+1 matching messages received, used in blockchain and mission-critical systems but with high message complexity O(nÂ²), modern variants like HotStuff and Tendermint improving performance with leader rotation and optimistic responsiveness, applications including Distributed Databases using Raft or Paxos for replicating data with strong consistency guaranteeing linearizability, Configuration Management with leader coordinating cluster membership changes safely, Distributed Locking electing lock owner and detecting failures, Leader Election selecting single coordinator for distributed tasks, and Metadata Management maintaining consistent view of system state, with properties including Safety guaranteeing agreement never violated where decided values never change, Liveness ensuring progress eventually made under favorable conditions requiring majority available and bounded network delays, Fault Tolerance continuing operation despite f failures in 2f+1 node cluster, and Performance considerations trading message complexity against latency typically 2-3 network round trips per decision, challenged by Network Partitions where minority partition cannot make progress until connectivity restored, Split-Brain scenarios prevented through quorum requirements ensuring at most one leader, and Configuration Changes requiring special procedures to safely add or remove nodes without violating quorum.
