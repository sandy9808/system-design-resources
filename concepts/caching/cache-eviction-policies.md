Cache Eviction Policies determine which items to remove when cache reaches capacity with LRU (Least Recently Used) evicting items not accessed for longest time tracking access timestamps maintaining linked list with O(1) operations using hashmap for lookup and doubly-linked list for ordering, working well for temporal locality where recently used items likely used again but requiring memory for metadata and suffering when access pattern is sequential scan, LFU (Least Frequently Used) evicting items with lowest access count tracking frequency using counters and min-heap providing good results when some items consistently more popular but struggling with new items having low count and older items retaining high counts despite becoming unpopular requiring decay mechanisms, FIFO (First In First Out) evicting oldest items regardless of usage requiring simple queue implementation with low overhead but ignoring access patterns potentially removing frequently used items simply because they're old, Random eviction selecting random item to remove requiring no metadata and providing surprising effectiveness for certain workloads with uniform access distribution but offering no guarantees and potentially poor performance for skewed access patterns, MRU (Most Recently Used) evicting most recently used items useful for sequential scans where recent items won't be accessed again but rare in practice outside specific access patterns, Clock (Second Chance) approximating LRU using circular buffer with reference bits giving entries second chance before eviction providing good balance of simplicity and effectiveness used in operating systems page replacement, ARC (Adaptive Replacement Cache) maintaining two LRU lists for recently used and frequently used items dynamically adapting between recency and frequency providing excellent hit rates but with higher complexity, and Segmented LRU (SLRU) dividing cache into segments based on access frequency promoting items between segments providing better hit rates than pure LRU with moderate overhead, modern implementations using approximations like TinyLFU combining frequency and recency filters to admit only valuable entries and W-TinyLFU using admission policy based on frequency estimate from count-min sketch requiring minimal memory overhead while achieving near-optimal hit rates, with selection depending on access patterns (temporal locality favors LRU, popularity-based favors LFU), memory constraints (simple FIFO vs complex ARC), hit rate requirements, and implementation complexity, often complemented by TTL-based expiration and probabilistic eviction for distributed caches.
